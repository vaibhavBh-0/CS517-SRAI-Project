{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from evaluate import load\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "judge_pipe = pipeline('text-generation', model=judge_model_id, torch_dtype=torch.bfloat16)\n",
    "# LLama 3.2 has multiple eos_token_id. We use the \"128001\"\n",
    "judge_pipe.tokenizer.pad_token_id = judge_pipe.model.config.eos_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipe = pipeline('text-generation', model=model_id, torch_dtype=torch.bfloat16)\n",
    "# LLama 3.2 has multiple eos_token_id. We use the \"128001\"\n",
    "pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id[0]\n",
    "\n",
    "_model = pipe.model\n",
    "_tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant8_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "quant8_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant8_config)\n",
    "quant8_pipe = pipeline('text-generation', model=quant8_model, tokenizer=_tokenizer, torch_dtype='auto')\n",
    "# LLama 3.2 has multiple eos_token_id. We use the \"128001\"\n",
    "quant8_pipe.tokenizer.pad_token_id = quant8_pipe.model.config.eos_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4')\n",
    "quant4_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant4_config)\n",
    "quant4_pipe = pipeline('text-generation', model=quant4_model, tokenizer=_tokenizer, torch_dtype='auto')\n",
    "# LLama 3.2 has multiple eos_token_id. We use the \"128001\"\n",
    "quant4_pipe.tokenizer.pad_token_id = quant4_pipe.model.config.eos_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_1b = sum(param.numel() * param.element_size() for param in _model.parameters()) / (1024 ** 2)\n",
    "llama_1b_int8 = sum(param.numel() * param.element_size() for param in quant8_model.parameters()) /  (1024 ** 2)\n",
    "llama_1b_int4 = sum(param.numel() * param.element_size() for param in quant4_model.parameters()) /  (1024 ** 2)\n",
    "\n",
    "print(f'LLama 3.2 1B Instruct with bfloat16 uses {round(llama_1b, 2)}MB of memory')\n",
    "print(f'LLama 3.2 1B Instruct with bfloat16 and int8 uses {round(llama_1b_int8, 2)}MB of memory')\n",
    "print(f'LLama 3.2 1B Instruct with bfloat16 and int4 uses {round(llama_1b_int4, 2)}MB of memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA open-ended short-form generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triviqa_dataset = load_dataset('mandarjoshi/trivia_qa', 'unfiltered', split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100 = triviqa_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_for_question(question: str) -> list:\n",
    "    \"\"\"\n",
    "    Get prompt in chat format. This includes a system and an user prompt.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {'role': 'system', 'content': 'You are a chatbot which answers user question in the most concise manner possible.'},\n",
    "        {'role': 'user', 'content': question}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100 = ds_100.add_column('question_prompt', column=list(map(get_prompt_for_question, ds_100['question'])))\n",
    "ds_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = 50\n",
    "\n",
    "# Select as per the GPU?\n",
    "MULTIPLIER = 100\n",
    "\n",
    "pipe_kwargs = {\n",
    "                'top_k': 0, \n",
    "                'top_p': 0.9, # Nucleus sampling: cumulative probability threshold\n",
    "                'max_new_tokens': 128,\n",
    "                'pad_token_id': _tokenizer.pad_token_id,\n",
    "                'batch_size': 5 * MULTIPLIER \n",
    "            }\n",
    "\n",
    "del MULTIPLIER\n",
    "\n",
    "# Originally, it's right side, but huggingface throws warning. \n",
    "# \"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
    "_tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "generations = {\n",
    "    'bfloat16': [],\n",
    "    'int8': [],\n",
    "    'int4': []\n",
    "}\n",
    "\n",
    "# inference_ds = KeyDataset(ds_100.repeat(SAMPLING), key='question')\n",
    "inference_ds = KeyDataset(ds_100.repeat(SAMPLING), key='question_prompt')\n",
    "n = len(inference_ds)\n",
    "\n",
    "with tqdm(total=n, desc='bfloat16') as pbar:\n",
    "    for out in pipe(inference_ds, **pipe_kwargs):\n",
    "        generations['bfloat16'].append(out[0]['generated_text'])\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "with tqdm(total=n, desc='int8') as pbar:\n",
    "    for out in quant8_pipe(inference_ds, **pipe_kwargs):\n",
    "        generations['int8'].append(out[0]['generated_text'])\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "with tqdm(total=n, desc='int4') as pbar:\n",
    "    for out in quant4_pipe(inference_ds, **pipe_kwargs):\n",
    "        generations['int4'].append(out[0]['generated_text'])\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_for_verification(question: str, answer: dict, chat_history: list) -> list:\n",
    "    \"\"\"\n",
    "    Get prompt in chat format. This includes a system and an user prompt.\n",
    "    \"\"\"\n",
    "    normalized_value = answer['normalized_value']\n",
    "    response = chat_history[-1]['content']\n",
    "    \n",
    "    return [\n",
    "        {'role': 'system', 'content': 'For the following query give response as True or False, nothing more.'}, # , nothing more\n",
    "        # {'role': 'user', 'content': f'For the question \"{ds_100[0]['question']}\", the correct answer is \"{ds_100[0]['answer']['normalized_value']}\". Does the response \"{responses[0]['generated_text'][-1]['content']} the time was Harry Truman\" the correct answer?'}\n",
    "        # {'role': 'user', 'content': f'Does \"{ds_100[0]['answer']['normalized_value']}\" appears in the following text \"{responses[0]['generated_text'][-1]['content']}\"'}\n",
    "        {'role': 'user', 'content': f'For the question \"{question}\", the correct answer is \"{normalized_value}\". Does the response \"{response}\" fits the correct answer?'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100_generations = ds_100.repeat(SAMPLING)\n",
    "\n",
    "for key, value in generations.items():\n",
    "    name = f'{key}_response'\n",
    "    ds_100_generations = ds_100_generations.add_column(name, value)\n",
    "    ds_100_generations = ds_100_generations.add_column(f'{key}_verification_prompt', column=list(map(get_prompt_for_verification, \n",
    "                                                                                                       ds_100_generations['question'], \n",
    "                                                                                                       ds_100_generations['answer'],\n",
    "                                                                                                       ds_100_generations[name])))\n",
    "\n",
    "ds_100_generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating responses on \"Correctness\" as seen by a judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select as per the GPU?\n",
    "MULTIPLIER = 35\n",
    "\n",
    "# params for Judge, no specific sampling methodology. LLama 3.2 3B follows the instructions quite well.\n",
    "verification_pipe_kwargs = {\n",
    "\n",
    "    'pad_token_id': _tokenizer.pad_token_id,\n",
    "    'batch_size': 5 * MULTIPLIER\n",
    "}\n",
    "\n",
    "del MULTIPLIER\n",
    "\n",
    "# Originally, it's right side, but huggingface throws warning. \n",
    "# \"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
    "# _tokenizer.padding_side = 'left'\n",
    "judge_pipe.tokenizer.padding_side = 'left'\n",
    "\n",
    "verifications = {\n",
    "    'bfloat16': [],\n",
    "    'int8': [],\n",
    "    'int4': []\n",
    "}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# We ask the judge multiple times and accumulate \"True\" or \"False\".\n",
    "VERIFICATION_SAMPLES = 25\n",
    "# VERIFICATION_SAMPLES = 10\n",
    "\n",
    "\n",
    "for key in verifications.keys():\n",
    "    verification_ds = KeyDataset(ds_100_generations.repeat(VERIFICATION_SAMPLES), key=f'{key}_verification_prompt')\n",
    "    n = len(verification_ds)\n",
    "\n",
    "    with tqdm(total=n, desc=f'{key}_verification') as pbar:\n",
    "        for out in judge_pipe(verification_ds, **verification_pipe_kwargs):\n",
    "            verifications[key].append(out[0]['generated_text'])\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations['bfloat16'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100_verifications_25 = ds_100_generations.repeat(VERIFICATION_SAMPLES)\n",
    "\n",
    "for key, value in verifications.items():\n",
    "    name = f'{key}_verification_response'\n",
    "    ds_100_verifications_25 = ds_100_verifications_25.add_column(name, value)\n",
    "\n",
    "ds_100_generations.save_to_disk('ds_100_slice')\n",
    "ds_100_verifications_25.save_to_disk('ds_100_verifications_25_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100_generations = load_from_disk('ds_100_slice')\n",
    "ds_100_verifications_25 = load_from_disk('ds_100_verifications_25_slice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample from tile to repeat (numpy style tile and repeat).\n",
    "example -> [1, 2, 3, 1, 2, 3] to [1, 1, 2, 2, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verification_key in ('bfloat16', 'int8', 'int4'): \n",
    "    verification_key = f'{verification_key}_verification_response'\n",
    "    arr = np.array([int(chat[-1]['content'].lower() == 'true') for chat in ds_100_verifications_25[verification_key]])\n",
    "    \n",
    "    # The first two dims are ordered following FIFO strategy to \n",
    "    # balance the 'tile' operations applied on the original 100 questions.\n",
    "    arr = arr.reshape(VERIFICATION_SAMPLES, SAMPLING, -1)\n",
    "    # We average the results over each verification for each sample.\n",
    "    mean_accuracy_per_sample : np.ndarray = arr.mean(axis=0)\n",
    "    # We then average and std over each sample for each question.\n",
    "    accuracy_per_question : np.ndarray = mean_accuracy_per_sample.mean(axis=0)\n",
    "    std_per_question : np.ndarray = mean_accuracy_per_sample.std(axis=0)\n",
    "\n",
    "    mean_accuracy = accuracy_per_question.mean(axis=0)\n",
    "    avg_std = std_per_question.mean(axis=0)\n",
    "\n",
    "    print(mean_accuracy, avg_std, arr.mean(), arr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy = 0.0\n",
    "\n",
    "for sample_question_idx in range(SAMPLING):\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Idx from the 50 * 100 questions.\n",
    "    # starting_idx = 1 + 100 * 49\n",
    "    original_question_idx = 0 \n",
    "    # sample_question_idx = 1\n",
    "    starting_idx = original_question_idx + 100 * sample_question_idx\n",
    "    for idx in range(starting_idx, n, len(ds_100_generations)):\n",
    "        item = verifications['bfloat16'][idx]\n",
    "        # print(item)\n",
    "        accuracy += int(item[-1]['content'].lower() == 'true')\n",
    "\n",
    "    # print('*' * 15)\n",
    "    # print(f'Accuracy is {(accuracy / count):.3f}%')\n",
    "    final_accuracy += accuracy / VERIFICATION_SAMPLES\n",
    "\n",
    "print(f'Accuracy is {(final_accuracy / SAMPLING):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_response_via_judge_model(ds: Dataset, idx: int, verification_samples: int = 25) -> list:\n",
    "    row = ds[idx]\n",
    "    question = row['question']\n",
    "    normalized_answer = row['answer']['normalized_value']\n",
    "\n",
    "    response_types = ['bfloat16_response', 'int8_response', 'int4_response']\n",
    "\n",
    "    response_accuracies = []\n",
    "\n",
    "    for key in response_types:\n",
    "        response = row[key]\n",
    "        # response = row[key].removeprefix(question)\n",
    "        # print(f'Question: {question}')\n",
    "        # print(f'Response: {response}')\n",
    "\n",
    "        response_item = [\n",
    "            {'role': 'system', 'content': 'For the following query give response as True or False, nothing more.'},\n",
    "            # {'role': 'user', 'content': f'Does \"{ds_100[0]['answer']['normalized_value']}\" appears in the following text \"{responses[0]['generated_text'][-1]['content']}\"'}\n",
    "            {'role': 'user', 'content': f'For the question \"{question}\", the correct answer is \"{normalized_answer}\". Does the response \"{response}\" fits the correct answer?'}\n",
    "        ]\n",
    "\n",
    "        verification_responses = judge_pipe([response_item] * verification_samples, pad_token_id = _tokenizer.pad_token_id)\n",
    "\n",
    "        print(json.dumps(verification_responses, indent=4))\n",
    "\n",
    "        accuracy_score = sum(int(veri_response[0]['generated_text'][-1]['content'].lower() == 'true') for veri_response in verification_responses) / verification_samples\n",
    "        response_accuracies.append(accuracy_score)\n",
    "\n",
    "    return response_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_response_via_judge_model(ds_100_generations, idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_accuracies = {\n",
    "    'bfloat16': [],\n",
    "    'int8': [],\n",
    "    'int4': []\n",
    "}\n",
    "\n",
    "for idx in tqdm(range(len(ds_100_generations))):\n",
    "    response_accuracies = verify_response_via_judge_model(ds_100_generations, idx=idx)\n",
    "    generation_accuracies['bfloat16'].append(response_accuracies[0])\n",
    "    generation_accuracies['int8'].append(response_accuracies[1])\n",
    "    generation_accuracies['int4'].append(response_accuracies[2])\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Bert Score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_metric = load('bertscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We choose the best F1 score from the TriviaQA dataset for each sample.\n",
    "So, we repeat the responses to match the references for the bert score metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_answers_count = [len(item['normalized_aliases']) for item in ds_100_generations['answer']]\n",
    "ds_100_generations = ds_100_generations.add_column('repeat_samples_count', possible_answers_count)\n",
    "\n",
    "# ds_100_generations.save_to_disk('ds_100_slice')\n",
    "\n",
    "ds_100_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_100_generations = load_from_disk('ds_100_slice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A possible problem with BERTScore???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_entries = []\n",
    "\n",
    "for item in ds_100_generations.select_columns(['question', 'answer', 'bfloat16_response', 'int8_response', 'int4_response', 'repeat_samples_count']):\n",
    "    prefix = item['question']\n",
    "    repeat_count = item['repeat_samples_count']\n",
    "    bfloat16_response = item['bfloat16_response']\n",
    "    normalized_ground_truth = item['answer']['normalized_aliases']\n",
    "    \n",
    "    bfloat16_scores = bert_score_metric.compute(predictions=[bfloat16_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en')\n",
    "    print(bfloat16_response.removeprefix(prefix))\n",
    "    print(normalized_ground_truth)\n",
    "    print(bfloat16_scores['f1'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = \"\"\"\n",
    " Harry Truman\n",
    "When the first Peanuts cartoon was published in 1950, President Harry Truman was in office.\n",
    "The cartoon was created by Charles M. Schulz and was first published in the Washington Post on October 2, 1950. It was later syndicated and became a huge success. The cartoon was originally called \"Li'l Folks\" and was later renamed \"Peanuts.\" It was known for its humorous and relatable portrayal of everyday life and its characters, including Charlie Brown, Snoopy, Lucy, and Linus. The cartoon was a huge success and became a cultural phenomenon, running for over 50\n",
    "\"\"\"\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = ds_100_generations['answer'][0]['value']\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_metric.compute(predictions=[prediction], references=[ds_100_generations['answer'][0]['value']], lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_entries = []\n",
    "batch_size = max(ds_100_generations['repeat_samples_count'])\n",
    "\n",
    "with tqdm(total=n, desc='Calculating BERT Score') as pbar:\n",
    "    for item in ds_100_generations.select_columns(['question', 'answer', 'bfloat16_response', 'int8_response', 'int4_response', 'repeat_samples_count']):\n",
    "        prefix = item['question']\n",
    "        repeat_count = item['repeat_samples_count']\n",
    "        bfloat16_response = item['bfloat16_response']\n",
    "        int8_response = item['int8_response']\n",
    "        int4_response = item['int4_response']\n",
    "        normalized_ground_truth = item['answer']['normalized_aliases']\n",
    "        # repeat_count = len(normalized_ground_truth)\n",
    "\n",
    "        # wrt normalized ground truth.\n",
    "        \n",
    "        bfloat16_scores = bert_score_metric.compute(predictions=[bfloat16_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en', batch_size=batch_size)\n",
    "        int8_scores = bert_score_metric.compute(predictions=[int8_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en', batch_size=batch_size)\n",
    "        int4_scores = bert_score_metric.compute(predictions=[int4_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en', batch_size=batch_size)\n",
    "\n",
    "        entry = {\n",
    "            'bfloat16': max(zip(bfloat16_scores['f1'], normalized_ground_truth)),\n",
    "            'int8': max(zip(int8_scores['f1'], normalized_ground_truth)),\n",
    "            'int4': max(zip(int4_scores['f1'], normalized_ground_truth))\n",
    "        }\n",
    "\n",
    "        bert_score_entries.append(entry)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfloat16_avg = sum([entry['bfloat16'][0] for entry in bert_score_entries]) / len(bert_score_entries)\n",
    "int8_avg = sum([entry['int8'][0] for entry in bert_score_entries]) / len(bert_score_entries)\n",
    "int4_avg = sum([entry['int4'][0] for entry in bert_score_entries]) / len(bert_score_entries)\n",
    "\n",
    "print(f'Following are the results:\\nbfloat16: {bfloat16_avg}\\nint8: {int8_avg}\\nint4: {int4_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_kwargs = {\n",
    "    'batch_size': 200\n",
    "}\n",
    "\n",
    "predictions_int8 = [response.removeprefix(question) for response, question in zip(ds_100_generations['int8_response'], ds_100_generations['question'])]\n",
    "predictions_int4 = [response.removeprefix(question) for response, question in zip(ds_100_generations['int4_response'], ds_100_generations['question'])]\n",
    "references_bfloat16 = [response.removeprefix(question) for response, question in zip(ds_100_generations['bfloat16_response'], ds_100_generations['question'])]\n",
    "\n",
    "int8_to_bfloat16_scores = bert_score_metric.compute(predictions=predictions_int8, references=references_bfloat16, lang='en', **bert_score_kwargs)\n",
    "int4_to_bfloat16_scores = bert_score_metric.compute(predictions=predictions_int4, references=references_bfloat16, lang='en', **bert_score_kwargs)\n",
    "\n",
    "avg_int8_bfloat16 = sum(int8_to_bfloat16_scores['f1']) / len(int8_to_bfloat16_scores['f1'])\n",
    "avg_int4_bfloat16 = sum(int4_to_bfloat16_scores['f1']) / len(int4_to_bfloat16_scores['f1'])\n",
    "\n",
    "print(avg_int8_bfloat16)\n",
    "print(avg_int4_bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_score_entries = []\n",
    "\n",
    "accumulated_predictions = {\n",
    "    'bfloat16': [],\n",
    "    'int8': [],\n",
    "    'int4': [],\n",
    "    'ground_truth': []\n",
    "}\n",
    "\n",
    "for item in ds_100_generations.select_columns(['question', 'answer', 'bfloat16_response', 'int8_response', 'int4_response', 'repeat_samples_count']):\n",
    "    prefix = item['question']\n",
    "    repeat_count = item['repeat_samples_count']\n",
    "    bfloat16_response = item['bfloat16_response']\n",
    "    int8_response = item['int8_response']\n",
    "    int4_response = item['int4_response']\n",
    "    normalized_ground_truth = item['answer']['normalized_aliases']\n",
    "    ground_truth = item['answer']['value']\n",
    "    # repeat_count = len(normalized_ground_truth)\n",
    "\n",
    "    # wrt normalized ground truth.\n",
    "\n",
    "    # accumulated_predictions['bfloat16'].extend([bfloat16_response.removeprefix(prefix)] * repeat_count)\n",
    "    # accumulated_predictions['int8'].extend([int8_response.removeprefix(prefix)] * repeat_count)\n",
    "    # accumulated_predictions['int4'].extend([int4_response.removeprefix(prefix)] * repeat_count)\n",
    "    # accumulated_predictions['ground_truth'].extend(normalized_ground_truth)\n",
    "\n",
    "    accumulated_predictions['bfloat16'].append(bfloat16_response.removeprefix(prefix))\n",
    "    accumulated_predictions['int8'].append(int8_response.removeprefix(prefix))\n",
    "    accumulated_predictions['int4'].append(int4_response.removeprefix(prefix))\n",
    "    # accumulated_predictions['ground_truth'].append(' '.join(normalized_ground_truth))\n",
    "    accumulated_predictions['ground_truth'].append(ground_truth)\n",
    "\n",
    "bert_score_kwargs = {\n",
    "    'batch_size': 200\n",
    "}\n",
    "\n",
    "bfloat16_scores = bert_score_metric.compute(predictions=accumulated_predictions['bfloat16'], references=accumulated_predictions['ground_truth'], lang='en', **bert_score_kwargs)\n",
    "int8_scores = bert_score_metric.compute(predictions=accumulated_predictions['int8'], references=accumulated_predictions['ground_truth'], lang='en', **bert_score_kwargs)\n",
    "int4_scores = bert_score_metric.compute(predictions=accumulated_predictions['int4'], references=accumulated_predictions['ground_truth'], lang='en', **bert_score_kwargs)\n",
    "    \n",
    "# bfloat16_scores = bert_score_metric.compute(predictions=[bfloat16_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en')\n",
    "# int8_scores = bert_score_metric.compute(predictions=[int8_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en')\n",
    "# int4_scores = bert_score_metric.compute(predictions=[int4_response.removeprefix(prefix)] * repeat_count, references=normalized_ground_truth, lang='en')\n",
    "\n",
    "avg_bfloat16_scores = sum(bfloat16_scores['f1']) / len(bfloat16_scores['f1'])\n",
    "avg_int8_scores = sum(int8_scores['f1']) / len(int8_scores['f1'])\n",
    "avg_int4_scores = sum(int4_scores['f1']) / len(int4_scores['f1'])\n",
    "\n",
    "print(avg_bfloat16_scores)\n",
    "print(avg_int8_scores)\n",
    "print(avg_int4_scores)\n",
    "\n",
    "# entry = {\n",
    "#     'bfloat16': max(zip(bfloat16_scores['f1'], normalized_ground_truth)),\n",
    "#     'int8': max(zip(int8_scores['f1'], normalized_ground_truth)),\n",
    "#     'int4': max(zip(int4_scores['f1'], normalized_ground_truth))\n",
    "# }\n",
    "\n",
    "# bert_score_entries.append(entry)\n",
    "\n",
    "    # Wrt bfloat16\n",
    "\n",
    "    # int8_to_bfloat16_scores = bert_score_metric.compute(predictions=[int8_response], references=[bfloat16_response], lang='en')\n",
    "\n",
    "    # print(int8_to_bfloat16_scores['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bert_score_metric.compute(predictions=['harry truman'] * 20, references=triviqa_dataset[0]['answer']['normalized_aliases'], lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_metric.compute(predictions=['harry truman'], references=[' '.join(triviqa_dataset[0]['answer']['normalized_aliases'])], lang='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SRAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
